{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a82f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from paddleocr import PaddleOCR,draw_ocr\n",
    "from tqdm import tqdm\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False, ocr_version=\"PP-OCRv3\", enable_mkldnn=True) # need to run only once to download and load model into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06ca56",
   "metadata": {},
   "source": [
    "## Line Table Extraction\n",
    "In this section, we develop code to extract the data table from the Line Image. \n",
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test2019\"\n",
    "data_path = '/dvmm-filer2/projects/mingyang/semafor/chart_table/data/line/annotations'\n",
    "test_data_path = '/'.join([data_path, \"instancesLine(1023)_{}.json\".format(split)])\n",
    "image_path = '/dvmm-filer2/projects/mingyang/semafor/chart_table/data/line/images/{}'.format(split)\n",
    "\n",
    "with open(test_data_path, 'r') as f:\n",
    "    test_annotation = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284217cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_annotation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcde079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_annotation['images']))\n",
    "print(type(test_annotation['annotations']))\n",
    "print(type(test_annotation['categories']))\n",
    "\n",
    "print(len(test_annotation['images']))\n",
    "print(len(test_annotation['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87362453",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_annotation['images'][0])\n",
    "print(test_annotation['annotations'][0])\n",
    "print(test_annotation['categories'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Line Annotation\n",
    "#image_info = [x['id']: {'file_name':x['file_name'], 'height':x['height'], 'width':x['width']} for x in test_annotation['images']]\n",
    "#good sample: [28]\n",
    "sample_info = test_annotation['images'][28]\n",
    "sample_id = sample_info['id']\n",
    "#Find all the annotations:\n",
    "\n",
    "#Display the image\n",
    "sample_img_path = '/'.join([image_path, sample_info['file_name']])\n",
    "display(Image(filename=sample_img_path))\n",
    "\n",
    "#Map Annotations to Image ID\n",
    "AnnotationMap = {}\n",
    "for x in test_annotation['annotations']:\n",
    "    img_id = x['image_id']\n",
    "    AnnotationMap[img_id] = AnnotationMap.get(img_id, []) + [x]\n",
    "    \n",
    "sample_annotation = AnnotationMap[sample_id]\n",
    "print(sample_annotation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3f114",
   "metadata": {},
   "source": [
    "### Detect Text Elements of the Image\n",
    "Extract the Text Elements from Line Image. \n",
    "\n",
    "1. **Extract Legend**\n",
    "    - Disregard the Numerical Values\n",
    "    - Disregard the bounding box that is not Vertically or Horizontally aligned\n",
    "    \n",
    "2. **Extract X-labels**\n",
    "   - Filter the texts that is above the 1/2 of the height (assume the x-axies is at the bottom)\n",
    "   - Using sliding line in vertical direction and record the intersected bounding box. \n",
    "   - The maximum number of intersection is considered as the x-labels\n",
    "   \n",
    "3. **Extract Y-labels** \n",
    "   - Filter the texts that is above the 1/2 of the height (assume the y-axies is to the left side)\n",
    "   - Using sliding line in vertical direction and record the intersected bounding box. \n",
    "   - The maximum number of intersection is considered as the y-labels\n",
    "\n",
    "\n",
    "2. **Extract Title**\n",
    "    - Detected Text that is closest to the Top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the OCR of this image\n",
    "ocr_dir = \"/dvmm-filer2/projects/mingyang/semafor/chart_table/data/line/ocr_results/test2019/{}.json\".format(sample_id)\n",
    "with open(ocr_dir, \"r\") as f:\n",
    "    ocr_texts = json.load(f)\n",
    "\n",
    "print(ocr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea413ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def ocr_preprocessing(ocr_texts):\n",
    "    filtered_ocr_texts = []\n",
    "    thred = 0.8\n",
    "    for line in ocr_texts:\n",
    "        if line[1][1] > thred:\n",
    "            filtered_ocr_texts.append(line)\n",
    "    return filtered_ocr_texts\n",
    "\n",
    "def hv_alignment(a, b):\n",
    "    #compare the bounding box a and b to check if they are horizontal or vertically aligned\n",
    "    align_thred = 5\n",
    "    align_diff = [abs(a[0][0]-b[0][0]), abs(a[0][1]-b[0][1]), abs(a[2][0]-b[1][0]), abs(a[2][1]-b[2][1])]\n",
    "    if min(align_diff) < align_thred:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "#Extract xlabels\n",
    "def extract_xlabels(ocr_texts, sample_info):\n",
    "    img_height = int(sample_info['height'])\n",
    "    #print(img_height)\n",
    "    x_label_candidates = []\n",
    "    \n",
    "    #Only consider the texts under the middle point. \n",
    "    starting_point = int(0.5*img_height)\n",
    "    #moving pace is the min of the bounding box size\n",
    "    step = int(min([x[0][2][1]-x[0][0][1] for x in ocr_texts]))\n",
    "    \n",
    "    intersected_list = []\n",
    "    for l_pos in range(starting_point, img_height, step):\n",
    "        #find the intersected texts\n",
    "        intersected_texts = [x for x in ocr_texts if x[0][0][1] < l_pos and x[0][2][1] > l_pos]\n",
    "        intersected_list.append(intersected_texts)\n",
    "    intersected_list = sorted(intersected_list, key=len)\n",
    "    x_label_candidates = intersected_list[-1]   \n",
    "        \n",
    "    return x_label_candidates\n",
    "\n",
    "#Extract ylabels\n",
    "def extract_ylabels(ocr_texts, sample_info):\n",
    "    img_width = int(sample_info['width'])\n",
    "    #print(img_height)\n",
    "    y_label_candidates = []\n",
    "    \n",
    "    #Only consider the texts under the middle point. \n",
    "    end_point = int(0.5*img_width)\n",
    "    #moving pace is the min of the bounding box width\n",
    "    step = int(0.5*min([x[0][2][0]-x[0][0][0] for x in ocr_texts]))\n",
    "    \n",
    "    \n",
    "    intersected_list = []\n",
    "    for l_pos in range(0, end_point, step):\n",
    "        #find the intersected texts\n",
    "        intersected_texts = [x for x in ocr_texts if x[0][0][0] < l_pos and x[0][2][0] > l_pos]\n",
    "        intersected_list.append(intersected_texts)\n",
    "    \n",
    "    intersected_list = sorted(intersected_list, key=len)\n",
    "    #print(intersected_list[-1])\n",
    "    y_label_candidates = intersected_list[-1]  \n",
    "    #Get rid of y_labels that are not numbers\n",
    "    \n",
    "    final_y_label_candidates = []\n",
    "    for y_label in y_label_candidates:\n",
    "        try:\n",
    "            y_label_text = y_label[1][0]\n",
    "            y_label_value = float(y_label[1][0])\n",
    "            final_y_label_candidates.append(y_label)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return final_y_label_candidates\n",
    "def estimate_range(y_labels):\n",
    "    \"\"\"\n",
    "    Make sure the order of the value is following the order of the position. \n",
    "    \"\"\"\n",
    "    #sorted(intersected_list, key=len)(key=x[0][0][1])\n",
    "    y_labels.sort(key=lambda x: x[0][0][1])\n",
    "    \n",
    "    y_max = max([float(y[1][0]) for y in y_labels])\n",
    "    y_max_candidate = float(y_labels[0][1][0])\n",
    "    while y_max != y_max_candidate:\n",
    "        #pop the first item\n",
    "        del y_labels[0]\n",
    "        y_max = max([float(y[1][0]) for y in y_labels])\n",
    "        y_max_candidate = float(y_labels[0][1][0])\n",
    "    y_max_pos = 0.5*(y_labels[0][0][0][1]+y_labels[0][0][2][1])\n",
    "    \n",
    "    y_min = min([float(y[1][0]) for y in y_labels])\n",
    "    y_min_candidate = float(y_labels[-1][1][0])\n",
    "    while y_min != y_min_candidate:\n",
    "        #pop the first item\n",
    "        del y_labels[-1]\n",
    "        y_min = min([float(y[1][0]) for y in y_labels])\n",
    "        y_min_candidate = float(y_labels[-1][1][0])\n",
    "    y_min_pos = 0.5*(y_labels[-1][0][0][1]+y_labels[-1][0][2][1])\n",
    "    \n",
    "    #y_values = [float(y[1][0]) for y in y_labels]\n",
    "    y_scale = (y_max - y_min)/(0.5*(y_labels[-1][0][0][1]+y_labels[-1][0][2][1]) - 0.5*(y_labels[0][0][0][1]+y_labels[0][0][2][1]))\n",
    "    return y_max, y_min, y_scale, y_max_pos, y_min_pos\n",
    "\n",
    "#Extract Legend\n",
    "def extract_legend(ocr_texts):\n",
    "    numerical_pattern = \"(\\d+(\\.\\d+))\"\n",
    "\n",
    "    #Step 1, Get rid of Numerical Value\n",
    "    legend_candidates = []\n",
    "    for candidate in ocr_texts:\n",
    "        candidate_text = candidate[1][0]\n",
    "        if not re.search(numerical_pattern, candidate_text):\n",
    "            legend_candidates.append(candidate)\n",
    "\n",
    "    #print(legend_candidates)\n",
    "    #Step 2, Group Text based on their Vertical or Horizontal Alignment.\n",
    "    align_map = {}\n",
    "    for i_a,c_a in enumerate(legend_candidates):\n",
    "        align_map[i_a] = []\n",
    "        i_b = i_a+1\n",
    "        for c_b in legend_candidates[i_a+1:]:\n",
    "            if hv_alignment(c_a[0],c_b[0]):\n",
    "                align_map[i_a].append(i_b)\n",
    "            i_b +=1\n",
    "    \n",
    "    legend_groups = []\n",
    "    legend_group = []\n",
    "\n",
    "    for k,v in align_map.items():\n",
    "        if k in legend_group:\n",
    "            continue\n",
    "        if v:\n",
    "            legend_group += v + [k]\n",
    "            for x in v:\n",
    "                legend_group += align_map[x]\n",
    "            legend_group = list(set(legend_group))\n",
    "            legend_groups.append(legend_group)\n",
    "            legend_group = []\n",
    "    if len(legend_groups) == 0:\n",
    "        return []\n",
    "    #Get the final legends\n",
    "    if len(legend_groups) == 1 and len(legend_groups[0]) > 0:\n",
    "        return [legend_candidates[x] for x in legend_groups[0]]\n",
    "    else:\n",
    "        #Pick the one with the maximum elements or maybe the group that has the mapped elements as the gt sectors\n",
    "        legend_groups.sort(key=len)\n",
    "        return [legend_candidates[x] for x in legend_groups[-1]]\n",
    "\n",
    "def expand_legend_box(detected_legends):\n",
    "    #Assume the mark is to the left\n",
    "    for c_legend in detected_legends:\n",
    "        width = c_legend[0][2][0]-c_legend[0][0][0]\n",
    "        expand_width = int(0.2*width)\n",
    "        c_legend[0][0][0] -= expand_width\n",
    "        c_legend[0][3][0] -= expand_width\n",
    "        \n",
    "ocr_texts = ocr_preprocessing(ocr_texts)\n",
    "#Get the x__labels\n",
    "x_labels = extract_xlabels(ocr_texts, sample_info)\n",
    "\n",
    "#Get the y_labels\n",
    "#print(\"\\n\")\n",
    "y_labels = extract_ylabels(ocr_texts, sample_info)\n",
    "\n",
    "#Estimate the Data Range Value\n",
    "y_max, y_min, y_scale, y_max_pos, y_min_pos= estimate_range(y_labels)\n",
    "\n",
    "#Get the legends\n",
    "filtered_ocr_texts = []\n",
    "for c_ocr in ocr_texts:\n",
    "    if c_ocr in x_labels or c_ocr in y_labels:\n",
    "        continue\n",
    "    filtered_ocr_texts.append(c_ocr)\n",
    "\n",
    "detected_legends = extract_legend(filtered_ocr_texts)\n",
    "\n",
    "print(detected_legends)\n",
    "#Augment the length of detected_legends\n",
    "expand_legend_box(detected_legends)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b24090",
   "metadata": {},
   "source": [
    "### Read Data Values\n",
    "In this step, we compute the value of each point based on its locations compared tot he y_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# def determinant(v1, v2):\n",
    "#   return v1[0]*v2[1] - v1[1]*v2[0]\n",
    "\n",
    "# def dotproduct(v1, v2):\n",
    "#   return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "# def length(v):\n",
    "#   return math.sqrt(dotproduct(v, v))\n",
    "\n",
    "# def angle(v1, v2):\n",
    "#   return math.atan2(determinant(v1,v2), dotproduct(v1, v2))\n",
    "\n",
    "def compute_unit(p1,p2):\n",
    "    distance = [p1[0] - p2[0], -(p1[1] - p2[1])]\n",
    "    norm = math.sqrt(distance[0] ** 2 + distance[1] ** 2)\n",
    "    direction = [distance[0] / norm, distance[1] / norm]\n",
    "    return direction\n",
    "\n",
    "def compute_distance(p1,p2):\n",
    "    distance = (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2\n",
    "    return distance\n",
    "    \n",
    "\n",
    "for line in sample_annotation:\n",
    "    line_pts = [(line['bbox'][i],line['bbox'][i+1])for i in range(0,len(line['bbox']),2)]\n",
    "    line_values = [0]*len(line_pts)\n",
    "    for i,pt in enumerate(line_pts):\n",
    "        pt_value = y_scale*(y_min_pos - pt[1]) + y_min\n",
    "        line_values[i] = pt_value\n",
    "    line['values'] = line_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc927e4",
   "metadata": {},
   "source": [
    "### Map the Legend or the Column Text\n",
    "In this final step we will map the legend text or the other detected Text to each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "def get_dominant_color(image, k=4, image_processing_size = None):\n",
    "    \"\"\"\n",
    "    takes an image as input\n",
    "    returns the dominant color of the image as a list\n",
    "    \n",
    "    dominant color is found by running k means on the \n",
    "    pixels & returning the centroid of the largest cluster\n",
    "\n",
    "    processing time is sped up by working with a smaller image; \n",
    "    this resizing can be done with the image_processing_size param \n",
    "    which takes a tuple of image dims as input\n",
    "\n",
    "    >>> get_dominant_color(my_image, k=4, image_processing_size = (25, 25))\n",
    "    [56.2423442, 34.0834233, 70.1234123]\n",
    "    \"\"\"\n",
    "    #resize image if new dims provided\n",
    "    if image_processing_size is not None:\n",
    "        image = cv2.resize(image, image_processing_size, \n",
    "                            interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    #reshape the image to be a list of pixels\n",
    "    image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "\n",
    "    #cluster and assign labels to the pixels \n",
    "    clt = KMeans(n_clusters = k)\n",
    "    labels = clt.fit_predict(image)\n",
    "\n",
    "    #count labels to find most popular\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    \n",
    "    #subset out most popular centroid\n",
    "    #dominant_color = clt.cluster_centers_[label_counts.most_common(1)[0][0]]\n",
    "    dominant_colors = clt.cluster_centers_\n",
    "    return list(dominant_colors)\n",
    "\n",
    "#If Legend Exist and the Length Matched\n",
    "#if detected_legends and len(detected_legends) == len(sample_annotation):\n",
    "#Get the dominant Color for each Legends\n",
    "legends = {}\n",
    "sample_image = cv2.imread(sample_img_path)\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "for i, dl in enumerate(detected_legends):\n",
    "    legends[i] = {'bbox': dl[0], 'text': dl[1]}\n",
    "    start_point = (int(dl[0][0][0]), int(dl[0][0][1]))\n",
    "    end_point = (int(dl[0][2][0]), int(dl[0][2][1]))\n",
    "    \n",
    "    cropped_image = sample_image[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.show()\n",
    "#     print(cropped_image[5,3])\n",
    "#     dominant_colors = get_dominant_color(cropped_image, k=3)\n",
    "#     print(dominant_colors)\n",
    "    legends[i]['cropped_image'] = cropped_image\n",
    "\n",
    "\n",
    "#Find the Color for Each Line, locate a center point, assume the legend will be there\n",
    "for line in sample_annotation:\n",
    "    line_pts = [(line['bbox'][i],line['bbox'][i+1])for i in range(0,len(line['bbox']),2)]\n",
    "    line['column_value'] = [\"None\"]*len(line_pts)\n",
    "    pick_point = line_pts[int(0.5*len(line_pts))]\n",
    "    pick_point = (int(pick_point[0]), int(pick_point[1]))\n",
    "    \n",
    "    sample_image_hsv = cv2.cvtColor(sample_image, cv2.COLOR_RGB2HSV)\n",
    "    pick_point_color = sample_image_hsv[pick_point[1],pick_point[0]]\n",
    "    \n",
    "    pp_color_min = np.array([max(0, pick_point_color[0]-5),max(50,pick_point_color[1]-10),max(50, pick_point_color[2]-10)], np.uint8)\n",
    "    pp_color_max = np.array([pick_point_color[0]+5,min(255, pick_point_color[1]+10),min(255, pick_point_color[2]+10)], np.uint8)\n",
    "    \n",
    "    line['row_value'] = \"None\"\n",
    "    for k, cl in legends.items():\n",
    "        cl_image = cl[\"cropped_image\"]\n",
    "        cl_hsv_image = cv2.cvtColor(cl_image, cv2.COLOR_RGB2HSV)\n",
    "        frame_threshed = cv2.inRange(cl_hsv_image, pp_color_min, pp_color_max)\n",
    "        \n",
    "\n",
    "        #might satisty two saturation\n",
    "        \n",
    "        if np.count_nonzero(frame_threshed)/(frame_threshed.shape[0]*frame_threshed.shape[1]) > 0.01:\n",
    "            #Find a Map\n",
    "            line['row_value'] = cl['text'][0]\n",
    "            break\n",
    "     #Map the line column values \n",
    "    print(line_pts)\n",
    "    for i, pt in enumerate(line_pts):\n",
    "        pt_x = pt[0]\n",
    "        thred = 5 #maybe set it to the length of the bbox\n",
    "        for x_label in x_labels:\n",
    "            x_label_text = x_label[1][0]\n",
    "            c_x = 0.5*(x_label[0][0][0]+x_label[0][2][0])\n",
    "            if abs(c_x-pt_x) < thred:\n",
    "                line['column_value'][i] = x_label_text\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "                             \n",
    "        \n",
    "\n",
    "                             \n",
    "    \n",
    "#plt.imshow('Binary',thresh_img)\n",
    "#print(sample_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=sample_img_path))\n",
    "for result in sample_annotation:\n",
    "    print(result['column_value'])\n",
    "    print(result['row_value'])\n",
    "    print(result['values'])\n",
    "#     for c, v in zip(result['column_value'], result['values']):\n",
    "#         print(f\"({c},{result['row_value']}): {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ea515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chart2table",
   "language": "python",
   "name": "chart2table"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a82f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.display import Image, display\n",
    "# from paddleocr import PaddleOCR,draw_ocr\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "# ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False, ocr_version=\"PP-OCRv3\", enable_mkldnn=True) # need to run only once to download and load model into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06ca56",
   "metadata": {},
   "source": [
    "## Pie Table Extraction\n",
    "In this section, we develop code to extract the data table from the Pie Image. \n",
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/dvmm-filer2/projects/mingyang/semafor/chart_table/data/pie/annotations'\n",
    "data_path = '/Users/kevinz/Desktop/semafor/chart_table/data_download/pie/annotations'\n",
    "test_data_path = '/'.join([data_path, \"instancesPie(1008)_test2019.json\"])\n",
    "#image_path = '/dvmm-filer2/projects/mingyang/semafor/chart_table/data/pie/images/test2019'\n",
    "image_path = '/Users/kevinz/Desktop/semafor/chart_table/data_download/pie/images/test2019'\n",
    "with open(test_data_path, 'r') as f:\n",
    "    test_annotation = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284217cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_annotation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcde079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_annotation['images']))\n",
    "print(type(test_annotation['annotations']))\n",
    "print(type(test_annotation['categories']))\n",
    "\n",
    "print(len(test_annotation['images']))\n",
    "print(len(test_annotation['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87362453",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_annotation['images'][0])\n",
    "print(test_annotation['annotations'][0])\n",
    "print(test_annotation['categories'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line Annotation\n",
    "#image_info = [x['id']: {'file_name':x['file_name'], 'height':x['height'], 'width':x['width']} for x in test_annotation['images']]\n",
    "#good sample: [21, 22, 23]\n",
    "sample_info = test_annotation['images'][24]\n",
    "sample_id = sample_info['id']\n",
    "#Find all the annotations:\n",
    "\n",
    "#Display the image\n",
    "sample_img_path = '/'.join([image_path, sample_info['file_name']])\n",
    "display(Image(filename=sample_img_path))\n",
    "\n",
    "#Map Annotations to Image ID\n",
    "AnnotationMap = {}\n",
    "for x in test_annotation['annotations']:\n",
    "    img_id = x['image_id']\n",
    "    AnnotationMap[img_id] = AnnotationMap.get(img_id, []) + [x]\n",
    "    \n",
    "sample_annotation = AnnotationMap[sample_id]\n",
    "print(sample_annotation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3f114",
   "metadata": {},
   "source": [
    "### Detect Text Elements of the Image\n",
    "Extract the Text Elements from Pie Image. \n",
    "\n",
    "1. **Extract Legend**\n",
    "    - Disregard the Numerical Values\n",
    "    - Disregard the bounding box that is not Vertically or Horizontally aligned\n",
    "2. **Extract Title**\n",
    "    - Detected Text that is closest to the Top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the OCR of this image\n",
    "ocr_dir = \"/Users/kevinz/Desktop/semafor/chart_table/data_download/pie/ocr_results/test2019/{}.json\".format(sample_id)\n",
    "with open(ocr_dir, \"r\") as f:\n",
    "    ocr_texts = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ocr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea413ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def hv_alignment(a, b):\n",
    "    #compare the bounding box a and b to check if they are horizontal or vertically aligned\n",
    "    align_thred = 5\n",
    "    align_diff = [abs(a[0][0]-b[0][0]), abs(a[0][1]-b[0][1]), abs(a[2][0]-b[1][0]), abs(a[2][1]-b[2][1])]\n",
    "    if min(align_diff) < align_thred:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "#Extract Legend\n",
    "def extract_legend(ocr_texts):\n",
    "    numerical_pattern = \"(\\d+(\\.\\d+)?%)\"\n",
    "\n",
    "    #Step 1, Get rid of Numerical Value\n",
    "    legend_candidates = []\n",
    "    for candidate in ocr_texts:\n",
    "        candidate_text = candidate[1][0]\n",
    "        if not re.search(numerical_pattern, candidate_text):\n",
    "            legend_candidates.append(candidate)\n",
    "\n",
    "\n",
    "    #Step 2, Group Text based on their Vertical or Horizontal Alignment.\n",
    "    align_map = {}\n",
    "    for i_a,c_a in enumerate(legend_candidates):\n",
    "        align_map[i_a] = []\n",
    "        i_b = i_a+1\n",
    "        for c_b in legend_candidates[i_a+1:]:\n",
    "            if hv_alignment(c_a[0],c_b[0]):\n",
    "                align_map[i_a].append(i_b)\n",
    "            i_b +=1\n",
    "    \n",
    "    legend_groups = []\n",
    "    legend_group = []\n",
    "\n",
    "    for k,v in align_map.items():\n",
    "        if k in legend_group:\n",
    "            continue\n",
    "        if v:\n",
    "            legend_group += v + [k]\n",
    "            for x in v:\n",
    "                legend_group += align_map[x]\n",
    "            legend_group = list(set(legend_group))\n",
    "            legend_groups.append(legend_group)\n",
    "            legend_group = []\n",
    "    if len(legend_groups) == 0:\n",
    "        return []\n",
    "    #Get the final legends\n",
    "    if len(legend_groups) == 1 and len(legend_groups[0]) > 0:\n",
    "        return [legend_candidates[x] for x in legend_groups[0]]\n",
    "    else:\n",
    "        #Pick the one with the maximum elements or maybe the group that has the mapped elements as the gt sectors\n",
    "        legend_groups.sort(key=len)\n",
    "        return [legend_candidates[x] for x in legend_groups[-1]]\n",
    "\n",
    "detected_legends = extract_legend(ocr_texts)\n",
    "print(detected_legends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b24090",
   "metadata": {},
   "source": [
    "### Read Data Values\n",
    "In this step, we compute the angles of the sector to estimate the pie area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def determinant(v1, v2):\n",
    "  return v1[0]*v2[1] - v1[1]*v2[0]\n",
    "\n",
    "def dotproduct(v1, v2):\n",
    "  return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "def length(v):\n",
    "  return math.sqrt(dotproduct(v, v))\n",
    "\n",
    "def angle(v1, v2):\n",
    "  return math.atan2(determinant(v1,v2), dotproduct(v1, v2))\n",
    "\n",
    "def compute_unit(p1,p2):\n",
    "    distance = [p1[0] - p2[0], -(p1[1] - p2[1])]\n",
    "    norm = math.sqrt(distance[0] ** 2 + distance[1] ** 2)\n",
    "    direction = [distance[0] / norm, distance[1] / norm]\n",
    "    return direction\n",
    "\n",
    "def compute_distance(p1,p2):\n",
    "    distance = (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2\n",
    "    return distance\n",
    "    \n",
    "#def estimate_value(sample_annotation):\n",
    "angles = []\n",
    "for sector in sample_annotation:\n",
    "    sector_bbox = sector['bbox']\n",
    "    #print(sector_bbox)\n",
    "    assert len(sector_bbox) == 6\n",
    "    center = [sector_bbox[4],sector_bbox[5]]\n",
    "    sector_p1 = [sector_bbox[0],sector_bbox[1]]\n",
    "    sector_p2 = [sector_bbox[2],sector_bbox[3]]\n",
    "    \n",
    "   \n",
    "   \n",
    "    v1 = compute_unit(sector_p1, center)\n",
    "    v2 = compute_unit(sector_p2, center)\n",
    "\n",
    "    current_angle = angle(v2,v1)\n",
    "    #print(current_angle)\n",
    "    if current_angle < 0:\n",
    "        current_angle = 2*math.pi+current_angle\n",
    "    \n",
    "    value = current_angle/(2*math.pi)\n",
    "    sector['value'] = value\n",
    "    print(value)\n",
    "#print(sample_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc927e4",
   "metadata": {},
   "source": [
    "### Map the Legend or the Column Text\n",
    "In this final step we will map the legend text or the other detected Text to each sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "def get_dominant_color(image, k=4, image_processing_size = None):\n",
    "    \"\"\"\n",
    "    takes an image as input\n",
    "    returns the dominant color of the image as a list\n",
    "    \n",
    "    dominant color is found by running k means on the \n",
    "    pixels & returning the centroid of the largest cluster\n",
    "\n",
    "    processing time is sped up by working with a smaller image; \n",
    "    this resizing can be done with the image_processing_size param \n",
    "    which takes a tuple of image dims as input\n",
    "\n",
    "    >>> get_dominant_color(my_image, k=4, image_processing_size = (25, 25))\n",
    "    [56.2423442, 34.0834233, 70.1234123]\n",
    "    \"\"\"\n",
    "    #resize image if new dims provided\n",
    "    if image_processing_size is not None:\n",
    "        image = cv2.resize(image, image_processing_size, \n",
    "                            interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    #reshape the image to be a list of pixels\n",
    "    image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "\n",
    "    #cluster and assign labels to the pixels \n",
    "    clt = KMeans(n_clusters = k)\n",
    "    labels = clt.fit_predict(image)\n",
    "\n",
    "    #count labels to find most popular\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    \n",
    "    #subset out most popular centroid\n",
    "    #dominant_color = clt.cluster_centers_[label_counts.most_common(1)[0][0]]\n",
    "    dominant_colors = clt.cluster_centers_\n",
    "    return list(dominant_colors)\n",
    "\n",
    "#If Legend Exist and the Length Matched\n",
    "#if detected_legends and len(detected_legends) == len(sample_annotation):\n",
    "#Get the dominant Color for each Legends\n",
    "legends = {}\n",
    "sample_image = cv2.imread(sample_img_path)\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "for i, dl in enumerate(detected_legends):\n",
    "    legends[i] = {'bbox': dl[0], 'text': dl[1]}\n",
    "    start_point = (int(dl[0][0][0]), int(dl[0][0][1]))\n",
    "    end_point = (int(dl[0][2][0]), int(dl[0][2][1]))\n",
    "    \n",
    "    cropped_image = sample_image[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "#     plt.imshow(cropped_image)\n",
    "#     print(cropped_image[5,3])\n",
    "#     dominant_colors = get_dominant_color(cropped_image, k=3)\n",
    "#     print(dominant_colors)\n",
    "    legends[i]['cropped_image'] = cropped_image\n",
    "\n",
    "\n",
    "#Find the Color for Each Sector, locate a center point. \n",
    "for sector in sample_annotation:\n",
    "    sector_bbox = sector['bbox']\n",
    "    #Compute the angle again\n",
    "    center = [int(sector_bbox[4]),int(sector_bbox[5])]\n",
    "    sector_p1 = [int(sector_bbox[0]),int(sector_bbox[1])]\n",
    "    sector_p2 = [int(sector_bbox[2]),int(sector_bbox[3])]\n",
    "   \n",
    "    v1 = compute_unit(sector_p1, center)\n",
    "    v2 = compute_unit(sector_p2, center)\n",
    "    \n",
    "   \n",
    "    current_angle = angle(v2,v1)\n",
    "    if current_angle < 0:\n",
    "        current_angle = 2*math.pi+current_angle\n",
    "    \n",
    "    \n",
    "    #rotate v2 counter-clock with for 1/2 current_angle to get the \n",
    "    rotated_v2_x = math.cos(0.5*current_angle)*v2[0] - math.sin(0.5*current_angle)*v2[1]\n",
    "    rotated_v2_y = math.sin(0.5*current_angle)*v2[0] + math.cos(0.5*current_angle)*v2[1]\n",
    "    \n",
    "    #print(rotated_v2_x**2 + rotated_v2_y**2)\n",
    "    radius = math.sqrt(compute_distance(sector_p1, center))\n",
    "    #Remember for Pixel, the y is increased from top to bottom. \n",
    "    pick_point = [int(center[0] + rotated_v2_x*0.5*radius), int(center[1] - rotated_v2_y*0.5*radius)]\n",
    "    \n",
    "    #Plot the pick-point\n",
    "    #Get the color of the pick_point\n",
    "    sample_image_hsv = cv2.cvtColor(sample_image, cv2.COLOR_RGB2HSV)\n",
    "    pick_point_color = sample_image_hsv[pick_point[1],pick_point[0]]\n",
    "    \n",
    "    \n",
    "    pp_color_min = np.array([max(0, pick_point_color[0]-5),max(50,pick_point_color[1]-10),max(50, pick_point_color[2]-10)], np.uint8)\n",
    "    pp_color_max = np.array([pick_point_color[0]+5,min(255, pick_point_color[1]+10),min(255, pick_point_color[2]+10)], np.uint8)\n",
    "    \n",
    "    sector['mapped_column_value'] = None\n",
    "    for k, cl in legends.items():\n",
    "        cl_image = cl[\"cropped_image\"]\n",
    "        cl_hsv_image = cv2.cvtColor(cl_image, cv2.COLOR_RGB2HSV)\n",
    "        frame_threshed = cv2.inRange(cl_hsv_image, pp_color_min, pp_color_max)\n",
    "        \n",
    "\n",
    "        #might satisty two saturation\n",
    "        \n",
    "        if np.count_nonzero(frame_threshed)/(frame_threshed.shape[0]*frame_threshed.shape[1]) > 0.01:\n",
    "            #Find a Map\n",
    "            sector['column_value'] = cl['text'][0]\n",
    "            plt.imshow(cl_image)\n",
    "            plt.show()\n",
    "            plt.imshow(frame_threshed)\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "        \n",
    "        \n",
    "                             \n",
    "        \n",
    "\n",
    "                             \n",
    "    \n",
    "#plt.imshow('Binary',thresh_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=sample_img_path))\n",
    "for result in sample_annotation:\n",
    "    print(f\"{result['column_value']}: {result['value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ea515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chart2table",
   "language": "python",
   "name": "chart2table"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}